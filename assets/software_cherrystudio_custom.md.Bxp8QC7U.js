import{b as g}from"./chunks/theme.BiY6TBf-.js";import{_ as u}from"./chunks/articleMetadata.BZXad0n-.js";import{_ as m,C as k,c as A,o as l,j as n,G as r,ah as v,a as y,w as o,b as p,e as d}from"./chunks/framework.97i6h3Kv.js";const I="/assets/custom-01.CAlh7DSl.png",C="/assets/custom-02.DpmuMpXf.png",b="/assets/custom-03.yXagZfdd.png",E=JSON.parse('{"title":"3-自定义服务商","description":"在 Cherry Studio 中添加您的自定义 AI 服务商","frontmatter":{"sort":3,"title":"3-自定义服务商","description":"在 Cherry Studio 中添加您的自定义 AI 服务商","date":"2025-04-01T22:11:20.000Z","tags":["Cherry-Studio"]},"headers":[],"relativePath":"software/cherrystudio/custom.md","filePath":"posts/software/cherrystudio/custom/README.md","lastUpdated":1743517396000}'),_={name:"software/cherrystudio/custom.md"};function F(s,t,L,f,P,M){const h=u,a=k("ClientOnly"),c=g;return l(),A("div",null,[t[0]||(t[0]=n("h1",{id:"自定义服务商",tabindex:"-1"},[y("自定义服务商 "),n("a",{class:"header-anchor",href:"#自定义服务商","aria-label":'Permalink to "自定义服务商"'},"​")],-1)),r(a,null,{default:o(()=>{var i,e;return[(((i=s.$frontmatter)==null?void 0:i.aside)??!0)&&(((e=s.$frontmatter)==null?void 0:e.showArticleMetadata)??!0)?(l(),p(h,{key:0,article:s.$frontmatter},null,8,["article"])):d("",!0)]}),_:1}),r(a,null,{default:o(()=>{var i;return[(i=s.$frontmatter)!=null&&i.articleGPT?(l(),p(c,{key:0})):d("",!0)]}),_:1}),t[1]||(t[1]=v('<p>Cherry Studio 不仅集成了主流的 AI 模型服务，还赋予了您强大的自定义能力。通过 <strong>自定义 AI 服务商</strong> 功能，您可以轻松接入任何您需要的 AI 模型。</p><h2 id="为什么需要自定义-ai-服务商" tabindex="-1">为什么需要自定义 AI 服务商？ <a class="header-anchor" href="#为什么需要自定义-ai-服务商" aria-label="Permalink to &quot;为什么需要自定义 AI 服务商？&quot;">​</a></h2><ul><li><strong>灵活性：</strong> 不再受限于预置的服务商列表，自由选择最适合您需求的 AI 模型。</li><li><strong>多样性：</strong> 尝试各种不同平台的 AI 模型，发掘它们的独特优势。</li><li><strong>可控性：</strong> 直接管理您的 API 密钥和访问地址，确保安全和隐私。</li><li><strong>定制化：</strong> 接入私有化部署的模型，满足特定业务场景的需求。</li></ul><h2 id="如何添加自定义-ai-服务商" tabindex="-1">如何添加自定义 AI 服务商？ <a class="header-anchor" href="#如何添加自定义-ai-服务商" aria-label="Permalink to &quot;如何添加自定义 AI 服务商？&quot;">​</a></h2><p>只需简单几步，即可在 Cherry Studio 中添加您的自定义 AI 服务商：</p><p><img src="'+I+'" alt="custom" loading="lazy"></p><ol><li><strong>打开设置：</strong> 在 Cherry Studio 界面左侧导航栏中，点击“设置”（齿轮图标）。</li><li><strong>进入模型服务：</strong> 在设置页面中，选择“模型服务”选项卡。</li><li><strong>添加提供商：</strong> 在“模型服务”页面中，您会看到已有的服务商列表。点击列表下方的“+ 添加”按钮，打开“添加提供商”弹窗。</li><li><strong>填写信息：</strong> 在弹窗中，您需要填写以下信息： <ul><li><strong>提供商名称：</strong> 为您的自定义服务商起一个易于识别的名称（例如：MyCustomOpenAI）。</li><li><strong>提供商类型：</strong> 从下拉列表中选择您的服务商类型。目前支持： <ul><li>OpenAI</li><li>Gemini</li><li>Anthropic</li><li>Azure OpenAI</li></ul></li></ul></li><li><strong>保存配置：</strong> 填写完毕后，点击“添加”按钮保存您的配置。</li></ol><h2 id="配置自定义-ai-服务商" tabindex="-1">配置自定义 AI 服务商 <a class="header-anchor" href="#配置自定义-ai-服务商" aria-label="Permalink to &quot;配置自定义 AI 服务商&quot;">​</a></h2><p><img src="'+C+'" alt="custom" loading="lazy"></p><p>添加完成后，您需要在列表中找到您刚刚添加的服务商，并进行详细配置：</p><ol><li><p><strong>启用状态</strong> 自定义服务商列表最右侧有一个启用开关，打开代表启用该自定义服务。</p></li><li><p><strong>API 密钥：</strong></p><ul><li>填写您的 AI 服务商提供的 API 密钥（API Key）。</li><li>点击右侧的“检查”按钮，可以验证密钥的有效性。</li></ul></li><li><p><strong>API 地址：</strong></p><ul><li>填写 AI 服务的 API 访问地址（Base URL）。</li><li>请务必参考您的 AI 服务商提供的官方文档，获取正确的 API 地址。</li></ul></li><li><p><strong>模型管理：</strong></p><ul><li>点击“+ 添加”按钮，手动添加此提供商下您想要使用的模型ID。例如 <code>gpt-3.5-turbo</code>、<code>gemini-pro</code> 等。</li></ul><p><img src="'+b+`" alt="custom" loading="lazy"></p><ul><li>如果您不确定具体的模型名称，请参考您的 AI 服务商提供的官方文档。</li><li>点击&quot;管理&quot;按钮，可以对已经添加的模型进行编辑或者删除。</li></ul></li></ol><h2 id="开始使用" tabindex="-1">开始使用 <a class="header-anchor" href="#开始使用" aria-label="Permalink to &quot;开始使用&quot;">​</a></h2><p>完成以上配置后，您就可以在 Cherry Studio 的聊天界面中，选择您自定义的 AI 服务商和模型，开始与 AI 进行对话了！</p><h2 id="使用-vllm-作为自定义-ai-服务商" tabindex="-1">使用 vLLM 作为自定义 AI 服务商 <a class="header-anchor" href="#使用-vllm-作为自定义-ai-服务商" aria-label="Permalink to &quot;使用 vLLM 作为自定义 AI 服务商&quot;">​</a></h2><p>vLLM 是一个类似Ollama的快速且易于使用的 LLM 推理库。以下是如何将 vLLM 集成到 Cherry Studio 中的步骤：</p><ol><li><p><strong>安装 vLLM：</strong> 按照 vLLM 官方文档（<a href="https://docs.vllm.ai/en/latest/getting_started/quickstart.html" target="_blank" rel="noreferrer">https://docs.vllm.ai/en/latest/getting_started/quickstart.html</a>）安装 vLLM。</p><div class="language-sh vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">sh</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> vllm</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"> # 如果你使用pip</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">uv</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> pip</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> vllm</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"> # 如果你使用uv</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div></li><li><p><strong>启动 vLLM 服务：</strong> 使用 vLLM 提供的 OpenAI 兼容接口启动服务。主要有两种方式，分别如下：</p><ul><li>使用<code>vllm.entrypoints.openai.api_server</code>启动</li></ul><div class="language-sh vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">sh</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -m</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> vllm.entrypoints.openai.api_server</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --model</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> gpt2</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><ul><li>使用<code>uvicorn</code>启动</li></ul><div class="language-sh vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">sh</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">vllm</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --model</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> gpt2</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --served-model-name</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> gpt2</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div></li></ol><p>确保服务成功启动，并监听在默认端口 <code>8000</code> 上。 当然， 您也可以通过参数<code>--port</code>指定 vLLM 服务的端口号。</p><ol start="3"><li><strong>在 Cherry Studio 中添加 vLLM 服务商：</strong><ul><li>按照前面描述的步骤，在 Cherry Studio 中添加一个新的自定义 AI 服务商。</li><li><strong>提供商名称：</strong> <code>vLLM</code></li><li><strong>提供商类型：</strong> 选择 <code>OpenAI</code>。</li></ul></li><li><strong>配置 vLLM 服务商：</strong><ul><li><strong>API 密钥：</strong> 因为 vLLM 不需要 API 密钥，可以将此字段留空，或者填写任意内容。</li><li><strong>API 地址：</strong> 填写 vLLM 服务的 API 地址。默认情况下，地址为： <code>http://localhost:8000/</code>（如果使用了不同的端口，请相应地修改）。</li><li><strong>模型管理：</strong> 添加您在 vLLM 中加载的模型名称。 在上面运行<code>python -m vllm.entrypoints.openai.api_server --model gpt2</code>的例子中, 应该在此处填入<code>gpt2</code></li></ul></li><li><strong>开始对话：</strong> 现在，您可以在 Cherry Studio 中选择 vLLM 服务商和 <code>gpt2</code> 模型，开始与 vLLM 驱动的 LLM 进行对话了！</li></ol><h2 id="提示与技巧" tabindex="-1">提示与技巧 <a class="header-anchor" href="#提示与技巧" aria-label="Permalink to &quot;提示与技巧&quot;">​</a></h2><ul><li><strong>仔细阅读文档：</strong> 在添加自定义服务商之前，请务必仔细阅读您所使用的 AI 服务商的官方文档，了解 API 密钥、访问地址、模型名称等关键信息。</li><li><strong>检查 API 密钥：</strong> 使用“检查”按钮可以快速验证 API 密钥的有效性，避免因密钥错误导致无法使用。</li><li><strong>关注 API 地址：</strong> 不同的 AI 服务商和模型，API 地址可能有所不同，请务必填写正确的地址。</li><li><strong>模型按需添加:</strong> 请只添加您实际上会用到的模型, 避免添加过多无用模型.</li></ul>`,20))])}const x=m(_,[["render",F]]);export{E as __pageData,x as default};
